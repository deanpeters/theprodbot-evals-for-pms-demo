# ðŸ§  TheProdBot Evals for PMs Demo

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deanpeters/theprodbot-evals-for-pms-demo/blob/main/TheProdBot_Evals_Demo.ipynb)

**A Productside hands-on demo** for product managers learning to run and evaluate AI reasoning systems â€” no data science degree required.  
This is the companion repo for the *How PMs 10Ã— Their Role with AI â€“ Part 2: Building Smarter* session.

Here's the link to the live, 13Nov25 webinar demonstrating how this notebook was created and is uses:
https://hubs.li/Q03Tk4fM0

---

## ðŸŽ¯ Purpose

Product managers donâ€™t need to *train* models â€” they need to *evaluate* them.  
This notebook walks you through using **evals** (AI acceptance criteria) to test how reasoning models handle multi-turn product discovery scenarios.

Youâ€™ll use a chatbot called **TheProdBot** to explore a market-sizing workflow:
> *â€œEstimate TAM â†’ SAM â†’ SOM for an AI PM assistant (think LennyBot or Teresa Torresâ€™ AI Interview Coach).â€*

---

## ðŸ§© How to Run It

### Option A â€” The 60-second setup (recommended)

1. Click **[Open in Colab](https://colab.research.google.com/github/deanpeters/theprodbot-evals-for-pms-demo/blob/main/TheProdBot_Evals_Demo.ipynb)**.  
2. In Colab, select **File â†’ Save a copy in Drive**.  
3. Run each cell in order:
   - âœ… Mount Google Drive  
   - ðŸ”‘ Enter your OpenAI API key (hidden input)  
   - ðŸš€ Run the TAMâ†’SAMâ†’SOM flow  
   - ðŸ§  Generate and review eval traces  
4. Use the **interactive labeler** to mark reasoning quality:
   - â€œgoodâ€ â†’ clear reasoning  
   - â€œweakâ€ â†’ incomplete logic or missing citations  
   - â€œfailâ€ â†’ hallucinated, math errors, or asked questions

Everything saves automatically to your Drive under:
```
/MyDrive/Colab Notebooks/TAM-SAM-SOM.Notebook/outputs
```

---

### Option B â€” Local clone (for advanced users)

```bash
git clone https://github.com/deanpeters/theprodbot-evals-for-pms-demo.git
cd theprodbot-evals-for-pms-demo
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
jupyter notebook TheProdBot_Evals_Demo.ipynb
```

---

## ðŸ§® What Youâ€™ll Learn

- How to design **multi-turn reasoning flows** for discovery.
- How to capture **context + prompt + response + reasoning**.
- How to generate **synthetic evals** across different models.
- How to review and label traces as a **human-in-the-loop evaluator**.
- How to interpret model quality trade-offs (3.5 vs 4.1, etc.).

---

## ðŸ“‚ Repo Structure

```
theprodbot-evals-for-pms-demo/
â”œâ”€â”€ TheProdBot_Evals_Demo.ipynb        # Main Google Colab notebook
â”œâ”€â”€ config_session.yaml                # Session + model configuration
â”œâ”€â”€ prompts_pm.json                    # Multi-turn prompt chain
â”œâ”€â”€ build_evals_dataset.py             # Generates synthetic evals
â”œâ”€â”€ build_traces.py                    # Builds human-readable traces
â”œâ”€â”€ export_traces_csv.py               # Outputs trace CSVs
â”œâ”€â”€ eval_labeler.py                    # Interactive labeling UI
â”œâ”€â”€ prompt_runner.py                   # Model selector + runner
â””â”€â”€ outputs/                           # (Auto-generated) results & logs
```

---

## ðŸ’¡ Why This Matters

Evals are product management, not data science.  
Theyâ€™re how PMs express what â€œgood reasoningâ€ looks like in measurable form â€”  
and how teams align model behavior with product intent.

> **Evals = acceptance criteria for AI.**

---

Â© 2025 Productside Â· Created by Dean Peters  
for *How PMs 10Ã— Their Role with AI â€“ Part 2: Building Smarter*  
[Productside.com](https://www.productside.com)
